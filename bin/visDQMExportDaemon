#!/usr/bin/env python3

import os, os.path, sys
from time import strftime, localtime, sleep, time
from Monitoring.Core.Utils import logme
from tempfile import mkstemp
from glob import glob
from threading import Thread, Lock, active_count
import subprocess, re
import pickle as pickle


INDEX = sys.argv[1]  # Location of the DQM GUI index from which we export samples.
WORKDIR = sys.argv[
    2
]  # Working directory of the current agent where cache and temporary files are stored.
FILEREPO = sys.argv[
    3
]  # Main file repository where all finished stream files will be moved.
FILTER = sys.argv[
    4
]  # Generic filter to select only specific samples based on predefined criteria. "True" will apply no selection.
NEXT = sys.argv[5:]  # Directory for the next agent in chain.
WAITTIME = 5  # Daemon cycle time.
MAXEXPORTTHREADS = (
    5  # Maximum number of concurrent thread to run to export sample out of the index
)
MAXDATFILES = 500  # Maximum number of .(dat|pb) files that can be present in the EXPORTDIR before pausing the streaming threads
MINDATFILES = 100  # Number of .(dat|pb) files that must be present in the EXPORTDIR before resuming the streaming threads
RXSAMPLEMATCH = "^SAMPLE\s+#(\d+).*src-file=#\d+.*?(DQM_V\d+_R\d+[A-Za-z0-9_-]*.root).*dataset-name=#\d+/((?:/[-A-Za-z0-9_]+){3}).*runnr=(\d+).*num-objects=(\d+)"
CACHEFILE = "%s/exportedSamples.dat" % WORKDIR
EXPORTDIR = "%s/Exported" % FILEREPO
OWNNAME = __file__.rsplit("/", 1)[-1]
MAXINDEXTIME = (
    60 * 60
)  # Maximum number of seconds that are given to following agents to cope with their pending jobs, besides .(dat|pb).dqminfo ones.
IDXESTIMATETIME = (
    2 * 60
)  # Average number of seconds of a typical indexing job on a standard root file.
MAXDATTIME = (
    60 * 60
)  # Maximum number of seconds that concurrent parallel stream jobs can run w/o pausing to process .root files.


# --------------------------------------------------------------------
# def logme(msg, *args):
#     procid = "[%s/%d]" % (OWNNAME, os.getpid())
#     sys.stdout.write(
#         "[%s/%s]" % (OWNNAME, os.getpid())
#         + strftime("%b %d %H:%M:%S ", localtime(time()))
#         + "".join(msg % args)
#         + "\n"
#     )


class Sample:
    """Information cached for each sample:

    id: this is the SAMPLE progressive number in the index whose content is
        exported.

    dataset name: the dataset name as registered in the index at indexing time.

    run number: the run number registerd in the index at indexing time.

    linked objects: the number of MEs that have been linked to the current
                    sample. It is used to remove deleted sample in the master
                    index.
    """

    id = 0
    dataset = ""
    runNumber = 0
    numObjects = 0
    streamFile = ""
    done = False

    def toInfo(self):
        """Format the information cached for each sample in a reduced info-file
        format to ease the handling of the .(dat|pb) files by subsequent agents."""
        info = {}
        info["dataset"] = self.dataset
        info["runnr"] = self.runNumber
        info["version"] = 1
        info["path"] = "Exported/%s" % self.streamFile
        info["class"] = "offline_data"
        return info


class Exporter(Thread):
    """Class responsible for exporting a sample. Each export operation
    will happen in a separate thread to improve performance. The maximum
    allowed number of concurrent threads is fixed to
    MAXEXPORTTHREADS. Each thread will first stream the assigned sample
    in a temporary area. Once the streaming is succesfully completed,
    the sample is moved into the central repository (EXPORTDIR). If the
    moving is succesfull, a basic dqminfo file is produced in the
    temporary directory and passed to the list of agents that follow the
    current one.  If all the above succeed, the central cache is updated
    and the current sample is marked as exported."""

    def __init__(self, sm, sample):
        Thread.__init__(self)
        self._sm = sm
        self._sample = sample

    def run(self):
        """Process a single sample out of the main index. It
        dumps its full content in a dedicated .(dat|pb) file, creates a
        simplyfied info file and drops it in the next agents DROPBOX."""

        logme("INFO: Processing sample %d" % self._sample.id)
        (status, msg) = subprocess.getstatusoutput(
            "cd %s && visDQMIndex streampb --sample %d %s"
            % (WORKDIR, self._sample.id, INDEX)
        )
        if status == 0:
            if self.moveSampleInPlace():
                if self.writeSampleInfo():
                    self._sample.done = True
                    logme("INFO: Updating cache for sample %d to done", self._sample.id)
                    self._sm.updateCache()
                else:
                    logme(
                        "ERROR: Failing to write info file for sample %d [%s]",
                        self._sample.id,
                        self._sample.streamFile,
                    )
                    if os.path.exists("%s/%s" % (EXPORTDIR, self._sample.streamFile)):
                        os.remove("%s/%s" % (EXPORTDIR, self._sample.streamFile))
            else:
                logme(
                    "ERROR: Failing to write .(dat|pb) file for sample %d [%s]",
                    self._sample.id,
                    self._sample.streamFile,
                )
        else:
            logme(
                "ERROR: Failing to stream sample %d [%s]\n%s",
                self._sample.id,
                self._sample.streamFile,
                msg,
            )

    def moveSampleInPlace(self):
        try:
            if os.path.exists("%s/%s" % (WORKDIR, self._sample.streamFile)):
                if not os.path.exists(EXPORTDIR):
                    os.makedirs(EXPORTDIR)
                os.rename(
                    "%s/%s" % (WORKDIR, self._sample.streamFile),
                    "%s/%s" % (EXPORTDIR, self._sample.streamFile),
                )
                return True
            else:
                return False
        except:
            return False

    def writeSampleInfo(self):
        try:
            if os.path.exists("%s/%s" % (EXPORTDIR, self._sample.streamFile)):
                finfo = "%s/%s.dqminfo" % (EXPORTDIR, self._sample.streamFile)
                try:
                    (fd, tmp) = mkstemp(dir=EXPORTDIR)
                    os.write(fd, str(self._sample.toInfo()))
                    os.close(fd)
                    os.rename(tmp, finfo)
                    for n in NEXT:
                        if not os.path.exists(n):
                            os.makedirs(n)
                        ninfo = "%s/%s.dqminfo" % (n, self._sample.streamFile)
                        if not os.path.exists(ninfo):
                            os.link(finfo, ninfo)
                    return True
                except:
                    if os.path.exists(tmp):
                        os.remove(tmp)
                    return False
            else:
                logme(
                    "ERROR: %s/%s file not found.", EXPORTDIR, self._sample.streamFile
                )
                return False
        except:
            return False


class SampleManager:
    """Class responsible of the handling of all the samples that need to
    be streamed out of the index. It has a private cache array which is
    filled with Sample objects. The updating of the cache by the
    streaming threads is controlled by the central lock variable held by
    this class, so that no concurrent modification of the cache can
    happen while it is written on disk."""

    def __init__(self):
        self._lock = Lock()
        self.SAMPLES = []

    def initialize(self):
        if not self.readFromCache():
            self.readFromIndex()

    def readFromCache(self):
        """Check for a cache file and, in case it is found, loads it into
        memory. Return true if the cache file was found and correctly
        loaded, False otherwise."""

        if len(self.SAMPLES):
            logme("ERROR: trying to load cache into a non-empty list of samples.")
            sys.exit(1)
        if not os.path.exists(CACHEFILE):
            logme("WARNING: No cache found, generating one.")
            return False
        try:
            with open(CACHEFILE, "rb") as _f:
                self.SAMPLES = pickle.load(_f)
            logme("INFO: Cache file loaded.")
            return True
        except:
            return False

    def readFromIndex(self):
        """Read the input index and prepare the list of samples that must
        be exported. The same list is written into a cache file and
        updated every time a file is correctly streamed. The agent will
        first try to read the cache and, in case of failures, will process
        the index."""

        cmd = "visDQMIndex dump %s catalogue" % INDEX
        samplesIndex = subprocess.getoutput(cmd).split("\n")
        for sample in samplesIndex:
            gr = re.match(RXSAMPLEMATCH, sample)
            if gr:
                s = Sample()
                s.id, s.streamFile, s.dataset, s.runNumber, s.numObjects = (
                    int(gr.group(1)),
                    gr.group(2).replace(".root", ".pb"),
                    gr.group(3),
                    int(gr.group(4)),
                    int(gr.group(5)),
                )
                if s.numObjects == 0:
                    logme(
                        "WARNING: discarding empty sample %d, %s, %s",
                        s.id,
                        s.dataset,
                        s.runNumber,
                    )
                select = False
                try:
                    select = eval(FILTER, {}, s.__dict__)
                except:
                    logme(
                        "WARNING: wrongly formatted filter %s.\nDiscarding all samples.",
                        FILTER,
                    )
                if select:
                    # Insert samples in reverse order, so that they will be processed in the correct order,
                    # from more recent to oldest.
                    self.SAMPLES.insert(0, s)
                else:
                    logme(
                        "WARNING: discarding sample %d, %s, %s",
                        s.id,
                        s.dataset,
                        s.runNumber,
                    )
        self.updateCache()

    def updateCache(self):
        self._lock.acquire()
        try:
            (fd, tmp) = mkstemp(dir="./")
            os.write(fd, pickle.dumps(self.SAMPLES))
            os.close(fd)
            os.rename(tmp, CACHEFILE)
            self._lock.release()
        except:
            self._lock.release()

    def maxNextPendingJobs(self):
        result = 0
        for n in NEXT:
            result = max(result, len(glob("%s/*.root.dqminfo" % n)))
        return result

    def numberExportedFiles(self, dir):
        types = ("*.dat", "*.pb")
        files = []
        for t in types:
            files.extend(glob("%s/%s" % (dir, t)))
        return len(files)

    def processSamples(self):
        """Loops over all samples and export each one. Stops when all
        samples have been streamed out. There are few conditions that are
        allowed to stop the streaming process so that the subsequent
        indexing job could cope with all the .(dat|pb) files and usual .root
        files. In particular if the number of .(dat|pb) files in the EXPORTDIR
        is greater than MAXDATFILES, the streaming threads are put on hold
        until the number of .(dat|pb) files drops below MINDATFILES. Once this
        number of .(dat|pb) files is reached in the EXPORTDIR, a check is done
        on all the dropboxes that have been passed as input parameters to
        this agents to see how many *.root.dqminfo files are present: if
        there are no pending jobs the streaming threads are resumed; if
        there are pending jobs, we stop the streaming threads taking the
        minimum between 60min and the estimated time to process the
        pending jobs."""

        now = time()
        for s in self.SAMPLES:
            maxNextPendingJobs = 0
            if s.done:
                continue
            if active_count() <= MAXEXPORTTHREADS:
                a_thread = Exporter(self, s)
                a_thread.start()
            while active_count() > MAXEXPORTTHREADS:
                sleep(2)
            if self.numberExportedFiles(EXPORTDIR) > MAXDATFILES:
                logme(
                    "INFO: Pausing streaming threads due to .(dat|pb) files quota exceeded."
                )
                while self.numberExportedFiles(EXPORTDIR) > MINDATFILES:
                    sleep(5)
                sleep(min(MAXINDEXTIME, IDXESTIMATETIME * self.maxNextPendingJobs()))
                now = time()
            if time() - now > MAXDATTIME:
                logme("INFO: Pausing streaming threads due time quota exceeded.")
                now = time()
                sleep(min(MAXINDEXTIME, IDXESTIMATETIME * self.maxNextPendingJobs()))
        logme("INFO: Finished processing all files")


sm = SampleManager()
sm.initialize()
sm.processSamples()
