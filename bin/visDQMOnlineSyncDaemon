#!/usr/bin/env python3

"""Usage: visDQMOnlineSyncDaemon [-s ONLINE-SERVER] [-n CONNECTIONS]
             [-d WAITTIME] DROPBOX FILEREPO NEXT

Synchronise ROOT file repository from ONLINE-SERVER to local FILEREPO.
Transfers all files appearing on ONLINE-SERVER which are not yet in
FILEREPO, or have different file size or modification time stamp than
the remote ones.

$X509_CERT_DIR and either $X509_USER_PROXY or $X509_USER_CERT/KEY must
be set correctly for authentication.
"""

from Monitoring.Core.HTTP import RequestManager
from Monitoring.Core.X509 import SSLOptions
import os, os.path, sys, re, hashlib, pycurl, urllib
from time import time, strptime, sleep
from optparse import OptionParser
from urllib import parse
from calendar import timegm
from datetime import datetime
from traceback import print_exc
from tempfile import mkstemp
from stat import *
sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 0)

# Directory where we receive input ("drop box"). Not used.
DROPBOX = None

# Final file repository of original DQM files.
FILEREPO = None

# Directories for the next agent in chain. This is where online info
# files are created, e.g. for zipping and transfer to tape.
NEXT = None

# Root URL for the server where we are retrieving contents, to be
# stripped off from file paths.
ROOT_URL = None

# Daemon cycle time. This should not be excessively frequent to avoid
# generating too much file-related load on the target online server.
WAITTIME = 1800

# Process umask, used to fix up temporary files after they are ok.
UMASK = None

# Object types.
DIR = 0
FILE = 1

# HTTP protocol `User-agent` identification string.
ident = "OnlineSync/1.0 python/%s.%s.%s" % sys.version_info[:3]

# SSL/X509 options.
ssl_opts = None

# HTTP request manager for content requests.
reqman = None

# Number of HTTP requests made for content.
nfetched = 0

# Number of files found in the scan.
nfound = 0

# Number of files copied on this round.
ncopied = 0

def logme(msg, *args):
  """Generate agent log message."""
  procid = "[%s/%d]" % (__file__.rsplit("/", 1)[-1], os.getpid())
  print(datetime.now(), procid, msg % args)

def myumask():
  """Get the current process umask."""
  val = os.umask(0)
  os.umask(val)
  return val

def need_to_copy(path, size, date):
  """Check if a remote file needs to be (re)copied to local repository.
  If the file exists, and has the same size and mtime, no copy is needed.
  Also, if file is missing but its dqminfo file exists asume it has been
  archived, therefore, no copy is needed ether.
  Otherwise requests the file to be copied."""
  try:
    info = os.stat(path)
    if int(info.st_mtime) == date and info.st_size == size:
      return False
  except:
    if os.path.exists("%s.dqminfo" % path):
      return False

  return True

def handle_init(c):
  """Prepare custom properties on download handles."""
  c.temp_file = None
  c.temp_path = None
  c.local_path = None

def request_init(c, options, kind, path, size, date):
  """`RequestManager` callback to initialise directory contents request."""
  # Set the download URL.
  assert c.temp_file == None
  assert c.temp_path == None
  assert c.local_path == None
  c.setopt(pycurl.URL, options.server + urllib.quote(path)
           + ((kind == DIR and path != "/" and "/") or ""))

  # If this is file download, prepare temporary destination file
  # in the target directory. process_task() will finish this off.
  if kind == FILE:
    local_path = "%s/%s" % (FILEREPO, path)
    dir, name = local_path.rsplit('/', 1)
    try:
      if not os.path.exists(dir):
        os.makedirs(dir, 0777 & ~UMASK)

      (fd, tmp) = mkstemp(dir=dir)
      fp = os.fdopen(fd, 'wb')
      c.setopt(pycurl.WRITEFUNCTION, fp.write)
      c.temp_file = fp
      c.temp_path = tmp
      c.local_path = local_path
      c.buffer = None
    except Exception as e:
      logme("ERROR: %s: %s", local_path, str(e))
      print_exc()

def cleanup(c):
  """Clean up file copy operation, usually after any failures."""
  if c.temp_file:
    try: c.temp_file.close()
    except: pass
  if c.temp_path:
    try: os.remove(c.temp_path)
    except: pass
  if c.local_path:
    try: os.remove(c.local_path)
    except: pass
  c.temp_file = None
  c.temp_path = None
  c.local_path = None
  c.buffer = None

def report_error(c, task, errmsg, errno):
  """`RequestManager` callback to report directory contents request errors."""
  global nfetched; nfetched += 1
  logme("ERROR: failed to retrieve %s %s from %s: %s (%d)",
        (task[1] == DIR and "directory") or "file", task[2],
	task[0].server, errmsg, errno)
  cleanup(c)

def process_task(c):
  """`RequestManager` callback to handle directory content response.

  This gets called once per every directory which has been successfully
  retrieved from the server. It parses the HTML response and turns it
  into object listing with all the file meta information.

  If verbosity has been requested, also shows simple progress bar on the
  search progress, one dot for every ten directories retrieved."""
  global nfetched, nfound, ncopied; nfetched += 1
  options, kind, path, size, date = c.task

  # First check if various basic info like HTTP response code.
  if c.getinfo(pycurl.HTTP_CODE) != 200:
    logme("ERROR: server responded with status %d for %s; skipping",
          c.getinfo(pycurl.HTTP_CODE), path)
    cleanup(c)
    return

  # If it's a FILE, process HTTP download. Finish saving the file contents
  # and set file mtime stamp.
  if kind == FILE:
    assert c.local_path, "Expected local path property to be set"
    assert c.temp_file, "Exepected temporary file property to be set"
    try:
      c.setopt(pycurl.WRITEFUNCTION, lambda *args: None)
      c.temp_file.close()
      c.temp_file = None

      if os.path.exists(c.local_path):
        os.remove(c.local_path)

      os.chmod(c.temp_path, 0666 & ~UMASK)
      os.utime(c.temp_path, (date, date))
      os.rename(c.temp_path, c.local_path)

      ncopied += 1
      c.local_path = None
      c.temp_path = None
      logme("INFO: downloaded %s", path)
    except Exception as e:
      logme("ERROR: downloading %s into %s failed: %s",
            path, c.local_path, str(e))
      print_exc()
    finally:
      cleanup(c)

  # If it's a DIR, scan directory contents and maybe fetch files.
  elif kind == DIR:
    assert c.temp_file == None, "Unexpected temporary file for a directory"
    assert c.temp_path == None, "Unexpected temporary path for a directory"
    assert c.local_path == None, "Unexpected local path for a directory"
    assert size == None, "Unexpected size for a directory"
    assert date == None, "Unexpected date for a directory"
    items = re.findall(r"<tr><td><a href='(.*?)'>(.*?)</a></td><td>(\d+|&nbsp;|-)</td>"
                       r"<td>(&nbsp;|\d\d\d\d-\d\d-\d\d \d\d:\d\d:\d\d UTC)</td>",
                       c.buffer.getvalue())

    for path, name, size, date in items:
      assert path.startswith(ROOT_URL)
      path = path[len(ROOT_URL):]

      if date == "&nbsp;":
        date = -1
      else:
        date = timegm(strptime(date, "%Y-%m-%d %H:%M:%S %Z"))

      if size == "&nbsp;" or size == "-":
        size = -1
      else:
        size = int(size)

      if path.endswith("/"):
        assert size == -1
        path = path[:-1]
        reqman.put((options, DIR, path, None, None))
      else:
        assert size >= 0
        if need_to_copy("%s/%s" % (FILEREPO, path), size, date):
          reqman.put((options, FILE, path, size, date))

  # Anything else is an internal implementation error.
  else:
    logme("ERROR: task type %s (%s) not recognised", kind, path)

# Parse command line options.
op = OptionParser(usage = __doc__)
op.add_option("-s", dest = "server",
              type = "string", action = "store", metavar = "SERVER",
              default = "https://cmsweb.cern.ch/dqm/online/data/browse",
              help = "Pull content from SERVER [default: %default]")
op.add_option("-n", dest = "connections",
              type = "int", action = "store", metavar = "N", default = 5,
              help = "Use N concurrent connections [default: %default]")
op.add_option("-d", dest = "delay",
              type = "int", action = "store", metavar = "N", default = WAITTIME,
              help = "Delay N seconds between iterations [default: %default]")
options, args = op.parse_args()
if len(args) < 2:
  sys.stderr.write("Too few arguments")
  sys.exit(1)
if not options.server:
  sys.stderr.write("Server contact string required")
  sys.exit(1)

UMASK = myumask()
ROOT_URL = parse.urlparse(options.server).path.rstrip('/')
DROPBOX = args[0]
FILEREPO = args[1]
NEXT = args[2:]

# Get SSL X509 parametres.
ssl_opts = SSLOptions()
logme("INFO: using SSL cert dir %s", ssl_opts.ca_path)
logme("INFO: using SSL private key %s", ssl_opts.key_file)
logme("INFO: using SSL public key %s", ssl_opts.cert_file)

# Start a request manager for contents.
reqman = RequestManager(num_connections = options.connections,
                        ssl_opts = ssl_opts,
                        user_agent = ident,
                        request_init = request_init,
                        request_respond = process_task,
                        request_error = report_error,
                        handle_init = handle_init)

# Process files forever.
while True:
  try:
    nfetched = nfound = ncopied = 0
    start = time()
    reqman.put((options, DIR, "/", None, None))
    reqman.process()
    end = time()

    logme("INFO: found %d directories, %d objects, %d copied in %.3f seconds",
          nfetched, nfound, ncopied, end - start)

  # If anything bad happened, barf but keep going.
  except KeyboardInterrupt as e:
    logme("INFO: exiting")
    map(cleanup, reqman.handles)
    sys.exit(0)

  except Exception as e:
    logme("ERROR: %s", e)
    print_exc()

  sleep(options.delay)
